<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
  <style>
    body {
      background-color: white;
      padding: 100px;
      width: 1000px;
      margin: auto;
      text-align: left;
      font-weight: 300;
      font-family: 'Open Sans', sans-serif;
      color: #121212;
    }

    h1,
    h2,
    h3,
    h4 {
      font-family: 'Source Sans Pro', sans-serif;
    }

    kbd {
      color: #121212;
    }

    table.custom-tbl {
      border: 1px solid;
    }

    table.custom-tbl th {
      border: 1px solid;
      background-color: rgb(99, 209, 209);
    }

    table.custom-tbl td {
      border: 1px solid;
      background-color: #f1e686a8;
    }
  </style>
  <title>CS 184 Path Tracer</title>
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

</head>


<body>

  <h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
  <h1 align="middle">Project 3-1: Path Tracer</h1>
  <h2 align="middle">Rahul Shah, Calvin Yan</h2>

  <!-- Add Website URL -->
  <h2 align="middle">Website URL: <a href="https://cal-cs184-student.github.io/project-webpages-sp23-rsha256/proj3-1/index.html">https://cal-cs184-student.github.io/project-webpages-sp23-rsha256/proj3-1/index.html</a></h2>

  <br><br>


  <div align="center">
    <table style="width=100%">
      <tr>
        <td align="middle">
          <img src="images/example_image.png" width="480px" />
          <figcaption align="middle">Bouncy Bunny</figcaption>
      </tr>
    </table>
  </div>

  <!-- <p>All of the text in your write-up should be <em>in your own words</em>.
    If you need to add additional HTML features to this document, you can search the <a
      href="http://www.w3schools.com/">http://www.w3schools.com/</a> website for instructions.
    To edit the HTML, you can just copy and paste existing chunks and fill in the text and image file names
    appropriately.</p>
  <o>The website writeup is intended to be a self-contained walkthrough of the assignment:
    we want this to be a piece of work which showcases your understanding of relevant concepts through both mesh images
    as well as written explanations about what you did to complete each part of the assignment.
    Try to be as clear and organized as possible when writing about your own output files or extensions to the
    assignment.
    We want to understand what you've achieved and how you've done it!</p>
    <p>If you are well-versed in web development, feel free to ditch this template and make a better looking page.</p>


    <p>Here are a few problems students have encountered in the past. Test your website on the instructional machines
      early!</p>
    <ul>
      <li>Your main report page should be called index.html.</li>
      <li>Be sure to include and turn in all of the other files (such as images) that are linked in your report!</li>
      <li>Use only <em>relative</em> paths to files, such as
        <pre>"./images/image.jpg"</pre>
        Do <em>NOT</em> use absolute paths, such as
        <pre>"/Users/student/Desktop/image.jpg"</pre>
      </li>
      <li>Pay close attention to your filename extensions. Remember that on UNIX systems (such as the instructional
        machines), capitalization matters.
        <pre>.png != .jpeg != .jpg != .JPG</pre>
      </li>
      <li>Be sure to adjust the permissions on your files so that they are world readable.
        For more information on this please see this tutorial: <a
          href="http://www.grymoire.com/Unix/Permissions.html">http://www.grymoire.com/Unix/Permissions.html</a></li>
      <li>And again, test your website on the instructional machines early!</li>
    </ul>


    <p>Here is an example of how to include a simple formula:</p>
    <p align="middle">
    <pre align="middle">a^2 + b^2 = c^2</pre>
    </p>
    <p>or, alternatively, you can include an SVG image of a LaTex formula.</p> -->

    <div>

      <h2 align="middle">Overview</h2>
      <p>
        In this project, we worked on the following features:
        <ol>
          <li>Part 1: Ray Generation and Scene Intersection. In this part we worked on the ray generation and primitive
            intersection parts of the rendering pipeline. We implemented the triangle intersection algorithm and showed
            images with normal shading for a few small <code>.dae</code> files.</li>
          <li>Part 2: Bounding Volume Hierarchy. In this part we implemented a bounding volume hierarchy to accelerate ray
            intersection tests. We showed images with normal shading for a few small <code>.dae</code> files.</li>
          <li>Part 3: Direct Illumination. In this part we implemented direct illumination using the Monte Carlo estimator.
            We showed images with normal shading for a few small <code>.dae</code> files.</li>
          <li>Part 4: Global Illumination. In this part we implemented global illumination using sampling with diffuse
            bidirectional scattering distribution function and finite recursion (thanks to Russian Roulette) for light
            estimation.</li>
          <li>Part 5: Adaptive Sampling. In this part we implemented adapative sampling to improve the quality of the
            image by reducing noise by using statistical analysis to determine the difficult parts of the image and
            concentrating the samples more there.</li>
        </ol>
        Overall, we have learned how to implement ray tracer methods from working with BVH Trees, 
        to understanding rays, 
        to understanding illumination, 
        to understanding the Monte Carlo estimator,
        to understanding converting recursion to iteration (EC),
        to understanding Russian Roulette,
        to understanding adaptive sampling,
        to understanding statistical analysis,
        along with furthering our understanding how to use the C++ standard library, 
        and overall having gained a better understanding of the rendering pipeline!
      </p>
      <br>

      <h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
      <!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

      <h3>
        Walk through the ray generation and primitive intersection parts of the rendering pipeline.
      </h3>
      <p>
        For the ray generation part of the rendering pipeline, we started out by finding the bounds of the screen
        which we did by utilizing the horizontal and vertical field of view aspect ratio
        which are stored as a member <code>double</code> variable in the <code>Camera</code>, which we then converted to
        <code>radians</code>.
        We then use the formulas detailed in the spec but modified to construct the following vector:
        $((2x-1)\tan(\frac{\text{hFov}}{2}), \ (2y-1)\tan(\frac{\text{vFov}}{2}), \ -1)$
        which we can then left-matmul with the 3x3 camera-to-world rotation matrix to get a normalized unit vector,
        which represents the direction of our ray.
        The constructred ray also has an origin at the camera position, and is what we return.

        For the primitive instruction parts of the rendering pipeline,
        we raytrace the respective pixel by sampling <code>num_samples</code> many different points uniformly
        (specifically it is uniform over a unit square via <code>UniformGridSampler2D::get_sample()</code>) across the
        pixel,
        and then we use the <code>camera</code>'s <code>generate_ray</code> method to generate a ray for each of them
        using the camera.
        We then average the radiance (found from <code>est_radiance_global_illumination</code>) from all of the rays and
        update.
      </p>
      <br>

      <h3>
        Explain the triangle intersection algorithm you implemented in your own words.
      </h3>
      <p>
        We used the Möller-Trumbore algorithm for computing the intersection of a triangle and a ray in three
        dimensional
        space. Given a ray with origin $o$ and direction $d$, and a triangle defined at vertices $p_0, p_1, p_2$,
        Möller-Trumbore attempts to find an analytical solution to
      </p>
      <p align="middle">$o + td = (1 - b_1 - b_2)p_0 + b_1p_1 + b_2p_2$</p>
      <p>For $t, b_1,$ and $b_2$.</p>
      <br>
      <p>
        If this is not possible, then the ray is on a plane parallel to the triangle and no intersection is found.
        Otherwise,
        the solution is guaranteed to give us the intersection of the ray with the triangle's plane, expressed as both a
        distance along the ray $t$ and barycentric coordinates $(1 - b_1 - b_2, b_1, b_2)$. Then the ray intersects the
        triangle if and only if the intersection with the point is interior to the triangle; this can be determined by
        confirming whether each of the barycentric coordinates is within $[0, 1]$.
      </p>
      <br>

      <h3>
        Show images with normal shading for a few small .dae files.
      </h3>
      <!-- Example of including multiple figures -->
      <div align="middle">
        <table style="width:100%">
          <tr align="center">
            <td>
              <img src="images/t1-CBspheres.png" align="middle" width="400px" />
              <figcaption>CBspheres.dae</figcaption>
            </td>
            <td>
              <img src="images/t1-CBcoil.png" align="middle" width="400px" />
              <figcaption>CBcoil.dae</figcaption>
            </td>
          </tr>
          <tr align="center">
            <td>
              <img src="images/t1-teapot.png" align="middle" width="400px" />
              <figcaption>teapot.dae (a little cameo from last project!)</figcaption>
            </td>
          </tr>
        </table>
      </div>
      <br>


      <h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
      <!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

      <h3>
        Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
      </h3>
      <p>
        Our BVH construction algorithm has a base case and a recursive case as it is a recursive algorithm. Formally, it
        has the following structure:
      <ol>
        <li>
          Base case: when there are <code>max_leaf_size</code> primitives or less in the list,
          we create a new <code>BVHNode</code> node and pass along the <code>start</code> and <code>end</code>
          <code>vector</code>s of <code>Primitive</code> pointers to the newly created node from what was passed into
          the <code>construct_bvh</code> method.
          We also set the <code>l</code> and <code>r</code> pointers to <code>NULL</code> to ensure there is no garbage
          in them before returning the newly created leaf node.
        </li>
        <li>
          Recursive case: when there are more than <code>max_leaf_size</code> primitives in the list, we need to divide
          the primitives among two children of the <code>BVHNode</code>. This is done using the heuristic of mean
          centroid. Formally, we compute the axis-aligned bounding box containing each primitive's centroid, 
          computing the mean of these centroids, and splitting the primitives by comparing their centroid to the mean along the
          bounding box's largest dimension.
        </li>
      </ol>
      Specifically we used the <code>std::partition</code> function to get the split point, using the heuristic of mean centroid, as detailed above. 
      Finally, we recurse on the '<i>left group</i>' which consists of those from start to the split point, as well as the '<i>right group</i>' 
      which consists of values greater than the heuristic or those from the split point to the end.
      </p>

      <h3>
        Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
      </h3>
      <!-- Example of including multiple figures -->
      <div align="middle">
        <table style="width:100%">
          <tr align="center">
            <td>
              <img src="images/t2-CBlucy.png" align="middle" width="400px" />
              <figcaption>CBlucy.dae</figcaption>
            </td>
            <td>
              <img src="images/t2-CBdragon.png" align="middle" width="400px" />
              <figcaption>CBdragon.dae</figcaption>
            </td>
          </tr>
          <tr align="center">
            <td>
              <img src="images/t2-blob.png" align="middle" width="400px" />
              <figcaption>blob.dae</figcaption>
            </td>
            <td>
              <img src="images/t2-maxplanck.png" align="middle" width="400px" />
              <figcaption>maxplanck.dae</figcaption>
            </td>
          </tr>
        </table>
      </div>
      <br>

      <h3>
        Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration.
        Present your results in a one-paragraph analysis.
      </h3>
      <p>
        We found significant speedups on rendering times (as seen by the difference being on orders of magnitudes
        apart).  Empirically, the time to render a scene appears linear in the average number of intersection tests per
        ray. Without BVH-based acceleration, the latter is linear in the number of primitives, which makes sense 
        because we are effectively casting every single ray on every single primitive in the scene. With BVH, the 
        average is significantly reduced, as we do not need to check approximately half the number of primitives in each level of recursion, 
        which leads to scaling logarithmically with the number of primitives. This can
        be explained by the ray traversing only part of the BVH tree when determining points of intersection.

        An unexpected result is <code> CBlucy.png </code> rendering the quickest despite being the most
        geometrically complex. A possible explanation is that the smaller area of the shape causes many rays to miss
        the bounding box of the BVH accelerator.
        
        Each of the benchmarks were performed on a resolution of 800 x 600.
      </p>
      <div align="center">
        <table class="custom-tbl">
          <tr>
            <th> Scene </th>
            <th> Number of Primitives </th>
            <th> Render time (no BVH) </th>
            <th> Render time (BVH) </th>
            <th> Average intersection tests per ray (no BVH) </th>
            <th> Average intersection tests per ray (BVH) </th>
          </tr>
          <tr>
            <td> <code>cow.png</code> </td>
            <td> 5856 </td>
            <td> 9.3192s </td>
            <td> 0.0505s </td>
            <td> 1139.9278 </td>
            <td> 3.8612 </td>
          </tr>
          <tr>
            <td> <code>maxplanck.png</code> </td>
            <td> 50801 </td>
            <td> 95.3548s </td>
            <td> 0.0621s </td>
            <td> 10956.7800 </td>
            <td> 4.4421 </td>
          </tr>
          <tr>
            <td> <code>CBlucy.png</code> </td>
            <td> 133796 </td>
            <td> 299.8749s </td>
            <td> 0.0327s </td>
            <td> 37231.0064 </td>
            <td> 1.9677 </td>
          </tr>
        </table>
      </div>
      <br>

      <h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
      <!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

      <h3>
        Walk through both implementations of the direct lighting function.
      </h3>
      <p>
        Direct lighting can be divided into zero-bounce illumination, which takes into account the light emitted
        directly from a light source to the camera without reflecting off the rest of the scene, and one-bounce
        illumination, consisting of the light that can reach the camera from the light source by reflecting off the
        scene exactly once. In either case, we begin by sampling "outgoing rays" from the scene to the camera plane at a
        position uniformly sampled from a grid. Zero-bounce illumination is somewhat straightforward to calculate, since
        we simply ask if the outgoing ray is reaching the camera from a light source. With one-bounce illumination, we 
        determine the amount of light reflected along the outgoing ray from its intersection with the scene, if one
        exists (otherwise we just move on). Since this requires an infinite integral which we cannot compute, we instead
        compute the Monte Carlo estimate of this reflectance as follows: 
        <ol>
        <li> Define $r_1$ as the outgoing ray and $i_1$ as the intersection of that ray with the scene (such that $r_1$)
             "bounces" off point $i_1$ into the camera.
        <li> Sample a ray $r_2$ uniformly from the hemisphere normal to $i_1$. 
        <li> If $i_1$ does not intersect the scene, return no light. Otherwise, compute the light reflected
              from $r_2$ (before the bounce) to $r_1$ (after the bounce) as follows:
              <p align="middle">$\frac{f(i_1, r_1 \rightarrow r_2) L_i(i_1, r_2) \cos \theta_j}{p(r_2)}$</p>
              Where $f(i_1, r_1 \rightarrow r_2)$ is the value of the BRDF from $r_2$ to $r_1$, $L_i(i_1, r_2)$ is the 
              radiance along $r_2$ incident to $i_1$, $\theta_j$ is the angle between $r_2$ and the surface normal at
              the intersection, and $p(r_2)$ is the pdf of sampling $r_2$.
        <li> Return the sample mean of the reflectance calculations.
        </ol>
      </p>
      <br>
      <p>
        Importance light sampling follows the same approach with slight modifications. Instead of sampling uniformly 
        over a hemisphere, the algorithm samples uniformly from each light source, considering only the rays that hit
        the point of intersection from the source. It samples one ray from each point source (since there is only one
        ray incident to a given point from such a source), and an arbitrary number of rays from each area source. It
        rejects "shadow rays" which are on the opposite side of the surface from the outgoing ray (we know this if the
        z-value of the shadow ray in object coordinates is negative), computes the sample mean reflectance for that 
        light source as described above, and then returns the mean of the sample means.
      </p>


      <h3>
        Show some images rendered with both implementations of the direct lighting function.
      </h3>
      <!-- Example of including multiple figures -->
      <div align="middle">
        <table style="width:100%">
          <!-- Header -->
          <tr align="center">
            <th>
              <b>Uniform Hemisphere Sampling</b>
            </th>
            <th>
              <b>Light Sampling</b>
            </th>
          </tr>
          <br>
          <tr align="center">
            <td>
              <img src="images/t3-CBspheres-H.png" align="middle" width="400px" />
              <figcaption>CBspheres_lambertian.dae</figcaption>
            </td>
            <td>
              <img src="images/t3-CBspheres-I.png" align="middle" width="400px" />
              <figcaption>CBspheres_lambertian.dae</figcaption>
            </td>
          </tr>
          <br>
          <tr align="center">
            <td>
              <img src="images/t3-dragon-H.png" align="middle" width="400px" />
              <figcaption>dragon.dae (This is to illustrate the failure of hemisphere sampling on point light sources!)</figcaption>
            </td>
            <td>
              <img src="images/t3-dragon-I.png" align="middle" width="400px" />
              <figcaption>dragon.dae</figcaption>
            </td>
          </tr>
          <br>
        </table>
      </div>
      <br>

      <h3>
        Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b>
        when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using
        light sampling, <b>not</b> uniform hemisphere sampling.
      </h3>
      <!-- Example of including multiple figures -->
      <div align="middle">
        <table style="width:100%">
          <tr align="center">
            <td>
              <img src="images/t3-CBbunny-l1.png" align="middle" width="200px" />
              <figcaption>1 Light Ray (CBbunny.dae)</figcaption>
            </td>
            <td>
              <img src="images/t3-CBbunny-l4.png" align="middle" width="200px" />
              <figcaption>4 Light Rays (CBbunny.dae)</figcaption>
            </td>
          </tr>
          <tr align="center">
            <td>
              <img src="images/t3-CBbunny-l16.png" align="middle" width="200px" />
              <figcaption>16 Light Rays (CBbunny.dae)</figcaption>
            </td>
            <td>
              <img src="images/t3-CBbunny-l64.png" align="middle" width="200px" />
              <figcaption>64 Light Rays (CBbunny.dae)</figcaption>
            </td>
          </tr>
        </table>
      </div>
      <p>
        A soft shadow represents a point from which a light source is partially, but not fully, occluded. These are the
        areas where low numbers of light rays generate the most noise, and where increasing the number of light rays
        reduces noise the most. One interpretation is that, because of the partial occlusion of the light source at 
        this point, light sampling demonstrates high variance; many rays from this source are obstructed, but many are 
        not. Thus, decreasing the number of light rays decreases sample size and increases variance, resulting in noise.
      </p>
      <br>

      <h3>
        Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
      </h3>
      <p>
        These results demonstrate the efficacy of lighting sampling as a tool for reducing noise and increasing
        performance in comparison to the more naive hemisphere sampling result. Not only do we observe little to no
        soft shadow noise in the lighting-sampled render than its hemisphere-sampled counterpart, but the former does so
        while tracing fewer rays overall, resulting in a significant speedup. Observe the results for the displayed 
        examples:

        <div align="center">
          <table class="custom-tbl">
            <tr>
              <th> Scene </th>
              <th> Rays traced (Hemisphere Sampling) </th>
              <th> Rays traced (Lighting Sampling) </th>
              <th> Render time (Hemisphere Sampling) </th>
              <th> Render time (Lighting Sampling) </th>
            </tr>
            <tr>
              <td> <code>CBspheres_lambertian.dae</code> </td>
              <td> 240053478 </td>
              <td> 98367505 </td>
              <td> 69.8184s </td>
              <td> 27.0214s </td>
            </tr>
            <tr>
              <td> <code>dragon.dae</code> </td>
              <td> 266229549 </td>
              <td> 97148287 </td>
              <td> 72.5541s </td>
              <td> 29.2156s </td>
            </tr>
          </table>
        </div>
        <br>

        <p>
        What lighting sampling is doing is "limiting" the search within the hemisphere to only those rays originating
        from a known light source. Thus, while it samples fewer rays than uniform sampling, more of these samples will
        produce light and thus contribute significantly to the overall reflectance. This causes sample variance and
        consequently noise to decrease.
        The ability to render point light sources is another advantage of lighting sampling. This effect is because, in 
        order for hemisphere sampling to sample from a point source, it would have to select the single "correct" ray with infinitesimally small probability.
        </p>

      </p>
      <br>


      <h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
      <!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

      <h3>
        Walk through your implementation of the indirect lighting function.
      </h3>
      <p>
        The algorithm for indirect lighting relies on the observation that $n$-bounce sampling is achievable by sampling
        a ray and conducting $(n - 1)$-bounce on the resulting hit point. From this we build our recursive approach.
        Given a sampled ray $r_1$ and its intersection $i_1$ with the scene:
        <ol>
          <li> Base case: if the ray's depth is zero, it has reached the maximum number of bounces allowed by us and we
              terminate.

          <li> Otherwise, compute the one-bounce illumination at this hit point, as defined above.
          <li> Perform Russian Roulette: given a continuation probability $cpdf$, terminate with probability $1 - cpdf$.
          <li> Compute the next "bounce" of the ray by sampling a direction from the distribution given by the BSDF at
               at the intersection and computing the resulting ray $r_2$'s intersection $i_2$ with the scene.
          <li> Set $i_2$'s depth to be one less than that of $i_1$, and recurse over $(r_2, i_2)$. 
          <li> Apply the reflectance formula to the result of recursion, add it to the total illumination, and return 
               it.

        </ol>
      </p>
      <br>

      <h3>
        Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
      </h3>
      <!-- Example of including multiple figures -->
      <div align="middle">
        <table style="width:100%">
          <tr align="center">
            <td>
              <img src="images/t4-CBspheres.png" align="middle" width="400px" />
              <figcaption>CBspheres_lambertian.dae</figcaption>
            </td>
            <td>
              <img src="images/t4-banana.png" align="middle" width="400px" />
              <figcaption>banana.dae</figcaption>
            </td>
          </tr>
        </table>
      </div>
      <br>

      <h3>
        Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination.
        Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to
        generate these views.)
      </h3>
      <!-- Example of including multiple figures -->
      <div align="middle">
        <table style="width:100%">
          <tr align="center">
            <td>
              <img src="images/t4-CBspheres-direct.png" align="middle" width="400px" />
              <figcaption>Only direct illumination (CBspheres_lambertian.dae)</figcaption>
            </td>
            <td>
              <img src="images/t4-CBspheres-indirect.png" align="middle" width="400px" />
              <figcaption>Only indirect illumination (CBspheres_lambertian.dae)</figcaption>
            </td>
          </tr>
        </table>
      </div>
      <br>
      <p>
        Both direct and indirect illumination present an incomplete picture of the render. Direct illumination consists
        only of zero-bounce rays (travelling directly from the light source to the camera) and one-bounce rays. Because
        the latter only illuminates those parts of the scene that rays from the light source can directly reach, which
        is not fully representative of the scene's appearance. The indirect illumination demonstrates what is missing, 
        including the undersides of 3D geometries, ceiling, and diffuse color. Adding the two renders gives us a 
        complete, globally illuminated scene.
      </p>
      <br>

      <h3>
        For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024
        samples per pixel.
      </h3>
      <!-- Example of including multiple figures -->
      <div align="middle">
        <table style="width:100%">
          <tr align="center">
            <td>
              <img src="images/t4-CBbunny-m0.png" align="middle" width="400px" />
              <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
            </td>
            <td>
              <img src="images/t4-CBbunny-m1.png" align="middle" width="400px" />
              <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
            </td>
          </tr>
          <tr align="center">
            <td>
              <img src="images/t4-CBbunny-m2.png" align="middle" width="400px" />
              <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
            </td>
            <td>
              <img src="images/t4-CBbunny-m3.png" align="middle" width="400px" />
              <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
            </td>
          </tr>
          <tr align="center">
            <td>
              <img src="images/t4-CBbunny-m100.png" align="middle" width="400px" />
              <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
            </td>
          </tr>
        </table>
      </div>
      <br>
      <p>
        Here we see that each additional bounce permitted in our global illumination estimation conveys more information
        about the scene. Maximum ray depths of 0 and 1 are equivalent to zero-bounce illumination and direct
        illumination, respectively. The former only illuminates the light source, while the latter fails to illuminate certain regions of the scene. Two-bounce rays introduces indirect illumination, while max ray depths of three 
        or more are able to diffuse color from the walls onto more of the scene. The differences become less noticeable 
        as the max depth becomes incredibly large; there are very few 100-bounce rays, for example, that convey more 
        information about the scene than a 3-bounce ray.
      </p>
      <br>

      <h3>
        Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8,
        16, 64, and 1024. Use 4 light rays.
      </h3>
      <!-- Example of including multiple figures -->
      <div align="middle">
        <table style="width:100%">
          <tr align="center">
            <td>
              <img src="images/t4-dragon-s1.png" align="middle" width="400px" />
              <figcaption>1 sample per pixel (dragon.dae)</figcaption>
            </td>
            <td>
              <img src="images/t4-dragon-s2.png" align="middle" width="400px" />
              <figcaption>2 samples per pixel (dragon.dae)</figcaption>
            </td>
          </tr>
          <tr align="center">
            <td>
              <img src="images/t4-dragon-s4.png" align="middle" width="400px" />
              <figcaption>4 samples per pixel (dragon.dae)</figcaption>
            </td>
            <td>
              <img src="images/t4-dragon-s8.png" align="middle" width="400px" />
              <figcaption>8 samples per pixel (dragon.dae)</figcaption>
            </td>
          </tr>
          <tr align="center">
            <td>
              <img src="images/t4-dragon-s16.png" align="middle" width="400px" />
              <figcaption>16 samples per pixel (dragon.dae)</figcaption>
            </td>
            <td>
              <img src="images/t4-dragon-s64.png" align="middle" width="400px" />
              <figcaption>64 samples per pixel (dragon.dae)</figcaption>
            </td>
          </tr>
          <tr align="center">
            <td>
              <img src="images/t4-dragon-s1024.png" align="middle" width="400px" />
              <figcaption>1024 samples per pixel (dragon.dae)</figcaption>
            </td>
          </tr>
        </table>
      </div>
      <br>
      <p>
        Observe the noise which decreases in intensity as the number of sampled rays per pixel increases. This is due
        to the inverse correlation of the variance of the Monte Carlo estimator with the sample size. In practical
        terms, the fewer samples we take at each pixel, the less representative the samples are of the illumination   
        from the whole light source (either too many shadow rays or not enough), giving us pixels that are brighter or
        darker than expected.
      </p>
      <br>


      <h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
      <!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. 
Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. 
Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, 
and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

      <h3>
        Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
      </h3>
      <p>
        In the previous part, we rendered a elegant images with the only downside being that they had a lot of noise.
        The immediate solution to this problem is to increase the number of samples per pixel.
        However, this is not always the best solution. As seen in the previous projects, 
        increasing the number of samples per pixel (aka increasing the sample rate) increases the render time at a much higher rate.
        This background information is important because it leads us to the next technique, Adaptive Sampling.
        Adaptive Sampling is a technique that allows us to render an image with a lower sample rate in areas that converge faster than others. 
        This allows us to render an image faster, while still getting a good result. 
        The way it works is we defined 2 <code>double</code>-precision variables <code>s1</code> and <code>s2</code> at the beginning of the program, 
        initialized to 0. Every iteration we update <code>s1</code> and <code>s2</code> as follows:
        $$s_1 = \sum_{k=1}^n x_k$$
        $$s_2 = \sum_{k=1}^n x_k^2$$
        for the $n$th iteration -- these 2 variables are updated every time a new sample is taken (each iteration).
        Chronologically, for every integer multiple of <code>samplesPerBatch</code> samples, we calculate:
        $$I = 1.96 \cdot \frac{\text{Stdev of the Luminance of the Light Samples}}{\sqrt{\text{Number of Samples}}} 
        = 1.96 \sqrt{\frac{\text{Variance of the Luminance of the Light Samples}}{\text{Number of Samples}}}$$
        and if $I \le \text{maxTolerance} \cdot\mu$, then we stop sampling that pixel, saving the iteration of sampling that pixel, 
        for averaging the color of that pixel correctly, in the process.
        This condition utilizes mean and variance of the luminance of the $n$ samples, which are updated every time $I$ is calculated.
        Specifically, for each ray that we casted, we find the radiance's <code>illum</code> which is $x_k$ for the $k$th sample.
        Then, we update the mean and variance of the luminance of the $n$ samples as follows:
        $$\mu = \frac{s_1}{n}$$
        $$\sigma^2 = \frac{1}{n - 1} \cdot \left(s_2 - \frac{s_1^2}{n}\right)$$
        where $\mu$ is the mean of the luminance of the $n$ samples, and $\sigma^2$ is the unbiased 
        (see https://en.wikipedia.org/wiki/Bessel%27s_correction) variance of the luminance of the $n$ samples.
      </p>
      <br>

      <h3>
        Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with
        clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate
        image, which shows your how your adaptive sampling changes depending on which part of the image you are
        rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
      </h3>
      <!-- Example of including multiple figures -->
      <div align="middle">
        <table style="width:100%">
          <tr align="center">
            <td>
              <img src="images/t5-bunny.png" align="middle" width="400px" />
              <figcaption>Rendered image (CBbunny.dae)</figcaption>
            </td>
            <td>
              <img src="images/t5-bunny_rate.png" align="middle" width="400px" />
              <figcaption>Sample rate image (CBbunny.dae)</figcaption>
            </td>
          </tr>
          <tr align="center">
            <td>
              <img src="images/t5-bench.png" align="middle" width="400px" />
              <figcaption>Rendered image (bench.dae)</figcaption>
            </td>
            <td>
              <img src="images/t5-bench_rate.png" align="middle" width="400px" />
              <figcaption>Sample rate image (bench.dae)</figcaption>
            </td>
          </tr>
          <tr align="center">
            <td>
              <img src="images/t5-banana.png" align="middle" width="400px" />
              <figcaption>Rendered image (banana.dae)</figcaption>
            </td>
            <td>
              <img src="images/t5-banana_rate.png" align="middle" width="400px" />
              <figcaption>Sample rate image (banana.dae)</figcaption>
            </td>
          </tr>
        </table>
      </div>
      In the above renderings, we can see that the adaptive sampling technique works well. 
      Furthermore, we can clearly see that it samples more in areas that are more complex 
      (vary more when exposed to lighting) 
      and less in areas that are simpler (such as the black backgrounds in the bottom 2 images).
      <br>

      <h2 align="middle">Extra Credit</h2>
      <h3>Challenge Level 1</h3>
      Here, we implemented more efficient construction and intersection routines for the BVH that replaced the recursive calls in <code>bvh.cpp</code>. 
      Specifically, we first updated <code>BVHAccel::construct_bvh</code> to construct the BVH using a stack and a while loop. 
      A key issue that we ran into here was that we needed to keep track of the current node that we were constructing 
      as well as a <code>std::vector<Primitive *>::iterator></code> that pointed to the current primitive that we were 
      constructing the BVH for wrt ranges. We solved this by using C++'s <code>std::make_pair</code> function to 
      allow us to store both of these values in a single <code>std::pair</code> object. Then, we replaced the base case 
      of the recursive call with an if statement in the while loop that checked the same condition and <code>continue</code>d if so. 
      Finally we updated the recursive call to push the next node and primitive range onto the stack instead. We ran into a lot of bugs here, 
      especially regarding the order in which we pushed the next node and primitive range onto the stack and the order in which we popped them off, 
      but we were able to solve them by carefully thinking through the algorithm and testing it on a few different scenes. 
      We also updated <code>BVHAccel::intersect</code> to use a stack and a while loop, and updated it in a similar manner as above.
      <br />
      <br />
      Performance metrics:<br />
      Recursion implementation: 0.0050 sec for construction time. 0.1818s for scene rendering time for <code>cow.dae</code>.
      <br />
      Iterative implementation: 0.0021 sec for construction time. 0.1301s for scene rendering time for <code>cow.dae</code>.
      <br />
      Here we can see that the iterative implementation is much faster than the recursive implementation. This makes sense as the iterative implementation 
      does not have the overhead of recursive function calls.

      <h3>Challenge Level 2</h3>
      Here we researched the $k$-D Tree as an alternative acceleration structure which we thought would be faster than the BVH. 
      A key reason for this is because $k$-D Trees are spatial partitioning whereas BVHs are object partitioning. However,
      that led us to the question of how to partition space into disjoint subsets. We found that the easiest way to do this is to 
      partition space into boxes of equal size.
      Take some measurements to compare performance of the two structures.
      <br />
      BVH Implementation: 0.0030 sec for construction time. 0.1555s for scene rendering time for <code>cow.dae</code>.<br />
      $k$-D Tree Implementation: 0.0089 sec for construction time. 0.2587s for scene rendering time for <code>cow.dae</code>.
      <br />
      We did not see a performance improvement here but I do think we were right in our inital assumption that the $k$-D Tree would be faster than the BVH. 
      If we had used the $k$-D Tree with smarter heuristics for choosing the split axis and split  point, 
      we would be more likely to have seen a performance improvement. 
      Alternatively, it is possible that $k$-D Trees by themselves are not dynamic enough, and if we had more time, 
      we would look into implementing hierarchical AABBs (axis-aligned bounding box).
      
      
      <h3>Challenge Level 3</h3>
      Here we offloaded workloads onto GPU by first renaming the <code>.cpp</code> file to a <code>.cu</code> and then adding the <code>__global__</code> keyword to the 
      functions that we wanted to run on the GPU. We then added the <code>__device__</code> keyword to any functions that we wanted to call from the GPU. 
      In addition to those keywords, 
      we also used the CUDA-specific syntax to allocate memory on the GPU and to transfer data between the CPU and GPU. 
      We did this by using the CUDA functions <code>cudaMalloc</code>, <code>cudaMemcpy</code>, and <code>cudaFree</code>. 
      Finally we launched kernels on the GPU using the <code><<<...>>></code> syntax, that Rahul was familiar with from taking CS C267. 
      This allowed us to accumulate radiance values estimated for each pixel in the sample buffer, using different threads for each pixel, 
      in an atomic fashion.
      
      <h2 align="middle">Contributions</h2>
      <!-- if you worked with a partner, 
        please write a short paragraph together for your final report that describes how you collaborated, 
        how it went, and what you learned. -->
      Calvin and Rahul contributed equally to this project. We collaborated on the project by meeting up 
      after lecture everyday and working on the project together. We also communicated through Discord and 
      worked asynchronously on tasks as needed to meet self-imposed deadlines. 
      We were able to do this with the help of VSCode's Live Share extension, which allowed us to work on
      the same codebase at the same time, despite being in different locations. 
      Overall, we had a great 
      experience working together and we learned a lot about how to work together on a project. Specifically, 
      we learned how to divide up tasks and work on them asynchronously, how to communicate effectively, and 
      how to reference lecture (especially for equations in algorithms such as Möller-Trumbore) 
      and ask specific questions in Office Hours 
      (where we coordinated at least one of us attending, for every OH since the project came out) 
      and EdStem to get another look at bugs.
</body>

</html>